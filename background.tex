\section{Background and Motivation} \label{sec:background}

There are two major approaches that attempt to solve the problem of computing on sensitive
datasets without disclosing micro-data: data enclaves and formal privacy techniques. Data enclaves attempt to
provide an environment that enables computation over sensitive data with security enforced
at the perimeter, thereby controlling what enters and exits the system. This approach often requires human oversight.
Formal privacy techniques, on the other hand, state privacy assumptions and guarantees and the privacy loss
from analyses are formally defined.

Formal privacy techniques, such as homomorphic encryption, secure multiparty computation, and
other cryptographic solutions for protecting private data, often require significant computational
overhead ~\cite{gentry2012fully} \cite{naehrig2011can}. Prior work on database implemented solutions
\cite{popa2011cryptdb} demonstrate the need for matching the privacy protecting techniques to the type of
numerical analyses in question to reduce overheads. Recent approaches for applying privacy techniques to
securely compute mathematical operations at large scale incur a 2x overhead~\cite{kepner2014computing}.
Nevertheless, the majority of the work in this area is focused on the security of numerical data, while the datasets
we focus on are mostly large volumes of unstructured and textual data.

Computing systems in some of the most critical and sensitive areas such as military, avionics,
and nuclear energy are protected from intrusion by physical isolation~\cite{byres2013air, ross2013security}.
For example, the U.S. Census Bureau operates several enclaves that host sensitive micro-data accessible
only on site~\cite{rdc_uscensus}. To overcome the limitations of physical proximity, data enclaves
have evolved to support network access via encrypted channels and limited bandwidth~\cite{lane2008using, grossman2016toward}.
Accessibility in this case comes at the cost of increased attack surface. Such systems, while
capable of limiting the extent to which data can be viewed, cannot protect against, for instance, a single
sensitive line of micro-data being viewed over a secure remote session. For this reason, they depend on non-disclosure
agreements with users to prevent such information from being divulged.

Traditionally data enclaves are hosted privately using on-premise infrastructure. As
such, they often require significant upfront investment (e.g. physical infrastructure) and recurring costs 
from the personnel required to operate the infrastructure as well as develop and maintain custom software. 
Moreover, and for this reason, they are also often limited in their ability to scale their
compute capabilities in response to requirements. In many cases, an on-premise data enclave is far from ideal
when considering distributed research teams, large-scale data analytics, and highly sporadic
usage patterns common in data and discovery science. Alternatively, cloud hosted data enclaves offer scalability,
democratize access to capabilities, and support distributed collaborations. 
 %terms of enabling anyone to host a secure data enclave and collaborate safely with a
%distributed set of peers.

\NAME is cloud hosted data-enclave designed to enable computation over sensitive datasets
in a cost effective and secure manner. In prior work we demonstrated the variety of ways in which \NAME offers
a scalable solution to match the compute needs of a distributed research network. \eamon{Cite everything??}
However, our extant model involved only two classes of user: 1) the administrators who take on the role of managing
data, approving access, and handling the transfer of data onto the data stores; and 2) the analysts
who are the consumers of data, performing the role of analyzing and interrogating data
via analysis pipelines.

\NAME currently manages over 10TB of compressed text data and in the past year
alone, its analyses have consumed over 0.25M core hours by a single team. With a cloud hosted system
such as \NAMENS, designed to enable large scale computational analyses over large datasets,
the overheads of formal privacy techniques are financially impractical. More importantly,
these techniques often do not lend themselves to secure analysis of arbitrary text data. 
The system thus protects the hosted micro-data by leveraging the strong security
guarantees of the cloud vendor's infrastructure and by utilizing strong access control
models. Some of the sensitive datasets managed in \NAME include JSTOR, Web of Science, IEEE,
and the UChicago grants database. Each has varying sensitivities and legal requirements associated with the
protection of microdata.

\NAMENS's datasets can be broadly categorized into three \cite{ist_dataclass}:
\begin{itemize}
\item \textbf{Public} : Datasets that are in the public domain and for which there is no
  reasonable expectation of sensitivity.
\item \textbf{Confidential}: Datasets that contain private information and for which there is a legal/contractual obligation to control access and usage.
\item \textbf {Regulated}: Datasets that contain sensitive data that can be widely damaging if disclosed.
  These are tightly controlled by government regulations.
\end{itemize}

The workflow of an analyst on \NAME involves requesting access to a dataset, submitting analysis
tasks via the various interfaces \cite{babuji2017enabling} to interrogate the data and tune the analysis until results are generated.
These results are available by accessing a unique URL for the submitted task and are not tied to any user.
This was designed to allow for sharing of analysis code and results, but it also introduced the risk
of unintended, unapproved access to the generated results. More importantly, since there are no disclosure
controls on the results generated, any sensitive data that is included in results can be exported.
In this paper, we describe how \NAME's security model has been extended to support the most sensitive datasets
that require stringent oversight with respect to data access access and export.



