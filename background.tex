\section{Background and Motivation} \label{sec:background}

There are two major approaches that attempt to solve the problem of computing on sensitive
datasets without disclosing micro-data: data enclaves and formal privacy techniques. Data enclaves attempt to
provide an environment that enables computation over sensitive data with security enforced
at the perimeter, contolling what enters and exits the system, often requiring human oversight.
Formal privacy techniques on the other hand state privacy assumptions and guarantees and the privacy loss
from analyses are formally defined.

\kyle{Any citations for the following}
Formal privacy techniques such as homomorphic encryption, secure multiparty computation, and
other cryptographic solutions for protecting private data often require significant computational
overhead ~\cite{gentry2012fully} \cite{naehrig2011can}. Prior work on database based solutions
\cite{popa2011cryptdb} demonstrate the need for matching the privacy protecting techniques to the type of
numerical analyses in question to reduce overheads. Recent approaches for applying privacy techniques to
securely compute mathematical operations at large scale incur a 2x overhead~\cite{kepner2014computing}.
The majority of the work in this area is focused on the security of numerical data, while the datasets
we focus on are mostly large volumes of unstructured and textual data.

Computing systems in some of the most critical and sensitive areas such as military, avionics,
and nuclear energy are protected from intrusion by physical isolation~\cite{byres2013air, ross2013security}.
For example, the U.S. Census Bureau operates several enclaves that host sensitive micro-data accessible
only on site~\cite{rdc_uscensus}. To overcome the limitations of physical proximity, data enclaves
have evolved to support network access via encrypted channels and limited bandwidth~\cite{lane2008using, grossman2016toward}.
Accessibility in this case comes at the cost of increased attack surface. Such systems, while
capable of limiting the extent to which data can be viewed, cannot protect against say a single
sensitive line of micro-data being viewed over a secure remote session. They depend on non-disclosure
agreements with users to prevent such information from being divulged.

Traditionally data enclaves are hosted privately using on-premise infrastructure. As 
such they often require significant upfront investment (e.g. physical infrastructure) and reoccurring costs 
from the personnel required to operate the infrastructure as well as develop and maintain custom software. 
They are also often limited in their ability to scale
compute capabilities in response to requirements. In many cases an on-premise data enclave is far from ideal
when considering distributed research teams, large-scale data anlaytics, and highly sporadic
usage patterns common in data science. Instead, cloud hosted data enclaves offer scalability,
democratize access to capabilities, and support distributed collaborations. 
 %terms of enabling anyone to host a secure data enclave and collaborate safely with a
%distributed set of peers.

\NAME is a data-enclave designed to enable computation over sensitive datasets
in a cost effective and secure manner. In prior work we've shown that \NAME offers
a scalable solution to match the compute needs of a distributed research network.
This model involved two classes of users: 1) the administrators who take on the role of managing
data, approving access, and handling the transfer of data onto the data stores; and 2) the analysts
who perform the role of analyzing and interrogating data via analyses pipelines.

\NAME currently manages over 10TB of compressed text data and in the past year
alone, its analyses have consumed over 0.25M core hours. With a cloud hosted system
such as \NAMENS, designed to enable large scale computational analyses over large datasets,
the overheads of formal privacy techniques are financially impractical. More importantly
these techniques often don't lend themselves to secure analysis of arbitrary text data. 
The system thus protects the hosted micro-data by leveraging the strong security
guarantees of the cloud vendor's infrastructure and by utilizing strong access control
models. The datasets managed in \NAME include JSTOR, Web of Science, IEEE,
and the UChicago grants database. Each has varying sensitivities and legal requirements to
protect the microdata.

\NAMENS's datasets can be broadly categorized into three \cite{ist_dataclass}:
\begin{itemize}
\item \textbf{Public} : Datasets that are in the public domain and there is no
  reasonable expectation of sensitivity.
\item \textbf{Confidential}: Datasets that contain private information and there is a legal/contractual obligation to control access and usage.
\item \textbf {Regulated}: Datasets that contain sensitive data that can be widely damaging if disclosed.
  These are tightly controlled by government regulations. Eg HIPAA, SSNs etc
\end{itemize}

The workflow of an analyst on \NAME involves requesting access to a dataset, submitting analysis
tasks via the various interfaces to interrogate the data and tune the analysis until results are generated.
These results are available by accessing a unique URL for the submitted task and are not tied to any user.
This was designed to allow for sharing of analysis codes and results, but it also introduces the risk
of unintended, unapproved access to the generated results. More importantly since there is no disclosure
controls on the results generated, any sensitive data that is included in results can be exported.
In this paper we describe how we have extended this security model to support the most sensitive datasets
that require stringent oversight with respect to data access access and export.



